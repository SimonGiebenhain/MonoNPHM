<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="MonoNPHM: Dynamic Head Reconstruction from Monoculuar Videos">
    <meta name="keywords" content="MonoNPHM, NPHM, morphable model, 3D reconstruction, Neural Fields, Virtual Avatars, Signed Distance Field, Deformation Field, 3D Scanning, Dataset">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MonoNPHM: Dynamic Head Reconstruction from Monoculuar Videos</title>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-E0HR9YQK2K"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-E0HR9YQK2K');
    </script>



    <!-- Global site tag (gtag.js) - Google Analytics -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <style>
		.render_wrapper {
			position: relative;
            height: 500px;
         }
        .render_wrapper_small {
			position: relative;
            height: 200px;
         }
		.render_div {
			position: absolute;
			top: 0;
			left: 0;
		}

        #interpolation-image-wrapper-car{
            text-align: center;
        }
        #interpolation-image-wrapper-chair{
            text-align: center;
        }
        .nested-columns {
            margin-bottom: 0 !important;
        }
    </style>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="https://unpkg.com/interactjs/dist/interact.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">MonoNPHM: Dynamic Head Reconstruction from Monoculuar Videos</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://simongiebenhain.github.io">Simon Giebenhain</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="https://niessnerlab.org/members/tobias_kirschstein/profile.html">Tobias Kirschstein</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="todo">Markos Georgopoulos</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.martinruenz.de/">Martin Rünz</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="">Lourdes Agapito</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://niessnerlab.org/members/matthias_niessner/profile.html">Matthias
                                    Nießner</a><sup>1</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Technical University of Munich,</span>
                            <span class="author-block"><sup>2</sup>Synthesia</span>
                            <span class="author-block"><sup>3</sup>University College London</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="static/MonoNPHM.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a class="external-link button is-normal is-rounded is-dark" href="https://arxiv.org/abs/2312.06740">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- Video Link. -->
                                <span class="link-block">
                                    <a href="https://www.youtube.com/watch?v=n-wjaC3UIeE"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <svg class="svg-inline--fa fa-youtube fa-w-18" aria-hidden="true"
                                                focusable="false" data-prefix="fab" data-icon="youtube" role="img"
                                                xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"
                                                data-fa-i2svg="">
                                                <path fill="currentColor"
                                                    d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z">
                                                </path>
                                            </svg><!-- <i class="fab fa-youtube"></i> Font Awesome fontawesome.com -->
                                        </span>
                                        <span>Video</span>
                                    </a>
                                </span>


                                <!-- Github Link. -->
                                <span class="link-block">
                                    <a class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code (coming soon)</span>
                                    </a>
                                </span>
                                <span class="link-block"></span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <img src="./static/teaser/teaser.jpg" height="150%" />
                <h2 class=" subtitle has-text-centered" style="padding-top: 10px">
                    <p align="left">
                    We present MonoNPHM, a neural-field-based parametric head model with disentangled latent space for geometry, expression and appearance (left).
                    MonoNPHM allows for 3D head tracking given only a monocular RGB video by optimizing for latent codes using SDF-based volumetric rendering (right).
                    </p>
                </h2>
            </div>
        </div>
    </section>





<!-- Result Overlay Videos-->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div style="text-align: center;" >
      <p style="font-size: 28px;">
          Tracking Results of <b>MonoNPHM</b>.
      </p>
      <p>
          From left to right: input, overlay, reconstrcutions.
      </p>

  </div>
      <div id="results-container" class="container results-container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/compressed_simon_510_s4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/compressed_simon_508_s5.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/compressed_simon_510_s3.mp4"
                    type="video/mp4">
          </video>
        </div>
          <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/compressed_simon_509_s4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/compressed_simon_511_s5.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>

  </div>
</section>






    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p margin="0">
                            We present Monocular Neural Parametric Head Models (MonoNPHM) for dynamic 3D head reconstructions from monocular RGB videos.
                            To this end, we propose a latent appearance space that parameterizes a texture field on top of a neural parametric model.
                            We constrain predicted color values to be correlated with the underlying geometry such that gradients from RGB effectively influence latent geometry codes during inverse rendering.
                            To increase the representational capacity of our expression space, we augment our backward deformation field with hyper-dimensions, thus improving color and geometry representation in topologically challenging expressions.
                            Using MonoNPHM as a learned prior, we approach the task of 3D head reconstruction using signed distance field based volumetric rendering.
                            By numerically inverting our backward deformation field, we incorporated a landmark loss using facial anchor points that are closely tied to our canonical geometry representation.
                            To evaluate the task of dynamic face reconstruction from monocular RGB videos we record 20 challenging Kinect sequences under casual conditions.
                            MonoNPHM outperforms all baselines with a significant margin, and makes an important step towards easily accessible neural parametric face models through RGB tracking.                    </div>
                </div>
            </div>

            <!-- Paper video. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Video</h2>
                    <div class="publication-video">
                        <iframe src="https://www.youtube.com/embed/n-wjaC3UIeE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
            <!--/ Paper video. -->
        </div>
    </section>

    <div style="text-align: center;" >
          <p style="font-size: 28px;">
              Reconstructed Geometry.
          </p>
      </div>
    <div class="container">
        <div id="results-carousel-mesh" class="carouselmesh results-carousel-mesh">
            <div class="item item-steve render_wrapper">
                <div id="mesh_chair_1" class="render_div"></div>
            </div>
            <div class="item item-steve render_wrapper">
                <div id="mesh_chair_2" class="render_div"></div>
            </div>
            <div class="item item-steve render_wrapper">
                <div id="mesh_chair_3" class="render_div"></div>
            </div>
            <div class="item item-steve render_wrapper">
                <div id="mesh_chair_4" class="render_div"></div>
            </div>
            <div class="item item-steve render_wrapper">
                <div id="mesh_chair_5" class="render_div"></div>
            </div>
        </div>
        <div style="text-align: center;">Press <b>R</b> to reset views. </div>
    </div>


    <section class="section">
        <div class="container is-max-desktop">
            <div class="container is-max-desktop">
                <div class="content has-text-justified">
                    <h2 class="title is-3">Latent Shape Interpolation</h2>
                      <p>
                Here is an interactive viewer allowing for latent identity interpolation.
                  Drag the <span style="color: #29e">blue cursor</span>
                  around to linearly interpolate between four different identites.
                  The resulting geometry and appearance is displayed on the right.
              </p>
                </div>
        <div class="columns is-centered is-vcentered">
          <div class="column is-one-third">
            <div class="hyper-space-wrapper-shape has-text-centered">
              <div class="hyper-space-axis-shape">
                <div class="hyper-space-shape-outer">
                    <div class="hyper-space-shape">
                        <div class="hyper-space-cursor-shape"></div>
                  </div>
                </div>
              </div>
              Latent Shape Coordinates
              <br/>
              <small>(Quadrilateral linear interpolation between 4 cornering identites.)</small>
            </div>
          </div>
          <div class="column is-two-thirds has-text-centered">
            <div class="hyper-grid-wrapper-shape">
              <div class="hyper-grid-rgb-shape">
                <img src="./static/figures/hyper_grid_shape.jpg"/>
              </div>
            </div>
            Resulting geometry and appearance in neutral expression.
          </div>
        </div>
      </div>


            <div class="container is-max-desktop">
        <div class="content has-text-justified">
            <br>
            <h2 class="title is-3">Latent Expression Interpolation</h2>
              <p>
                Here is an interactive viewer allowing for latent expression interpolation for a fixed identity.
                  Drag the <span style="color: #29e">blue cursor</span>
                  around to linearly interpolate between four expressions.
                  The resulting geometry and appearance is displayed on the right.
              </p>
        </div>


        <div class="columns is-centered is-vcentered">
          <div class="column is-one-third">
            <div class="hyper-space-wrapper has-text-centered">
              <div class="hyper-space-axis">
                <div class="hyper-space-outer">
                  <div class="hyper-space">
                    <div class="hyper-space-cursor"></div>
                  </div>
                </div>
              </div>
              Latent Expression Coordinates
              <br/>
              <small>(Quadrilateral linear interpolation between 4 cornering expressions.)</small>
            </div>
          </div>
          <div class="column is-two-thirds has-text-centered">
            <div class="hyper-grid-wrapper">
              <div class="hyper-grid-rgb">
                <img src="./static/figures/hyper_grid_sq.jpg"/>
              </div>
            </div>
            Posed Geometry and Appearance.
          </div>
        </div>
      </div>
      </div>

    </section>



    <section class="hero is-light is-small">

    <div class="hero-body">
        <div style="text-align: center;" >
              <p style="font-size: 28px;">
                  Expression Transfers.
              </p>
              <p>
                  Tracked expression codes from the input video (left) are transferred to other identites in the right.
              </p>

          </div>
    <div id="results-container" class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/compressed_expression_transfer_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/compressed_expression_transfer_2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/compressed_expression_transfer_3.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    </div>
</section>

    <section class="section">
        <div class="container is-max-desktop">

    <!-- Overview. -->
    <div class="columns is-centered" style="margin-top: 15px">
        <div class="column is-full-width">
            <h2 class="title is-3">Method Overview</h2>
            <img src="./static/teaser/overview.jpg"/>
            <div class="content has-text-justified" style="padding-top: 15px">
                <p><b>1.</b> Given a point in posed space, we backward-warp it into canonical space.</p>
                <p><b>2.</b> Geometry is represented in canonical space using a neural SDF, which also produces a condition for the appearance.</p>
                <p><b>3.</b> Appearance is modeled using a texture field in canonical space. Conditioning on geometry features ensures more effective gradients from an RGB loss to the geometry code during inverse rendering.</p>
                <p><b>4.</b> Our geometry and appearance networks depend on a set of discrete face anchor points. Using iterative root finding, we can numerically invert the backward deformation field to obtain anchors in posed space.</p>
                <p><b>5.</b> We perform tracking by optimizing for latent geometry, appearance and expression parameters. RGB and silhouette losses are built using deformable volumetric rendering. The landmark loss is computed by projecting posed anchors into image space.</p>
            </div>

        </div>
    </div>

<h2 class="title is-3">Deformation Consistency</h2>
            We visualize the tracked geometry (left), xy-canonical coordinates (middle), and the predicted hyper dimensions mapped to the red and green channel (right).
<div class="hero-body">
    <div class="container">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/compressed_canonical_coordinates.mp4"
                    type="video/mp4">
          </video>

    </div>
</div>


    <div class="columns is-centered">
        <div class="column is-full-width">
            <h2 class="title is-3">Related Links</h2>

            <div class="content has-text-justified">
                <p>
                    <b>
                        For more work on similar tasks, please check out the following papers.
                    </b>
                </p>
                <p>
                    <a href="https://simongiebenhain.github.io/NPHM/">NPHM</a> and <a href="https://github.com/MingwuZheng/ImFace">IMface</a> learn neural parametric model for facial geometry.
                </p>
                <p>
                    Neural parametric models including color were proposed in <a href="https://arxiv.org/abs/2212.07275">PhMoH</a> and
                    <a href="https://research.nvidia.com/labs/toronto-ai/ssif/">SSIF</a>.
                </p>



                <p>
                    <b>
                        For more work utilizing NPHMs see:
                    </b>
                </p>
                <p>
                    <a href="https://tobias-kirschstein.github.io/diffusion-avatars/">DiffusionAvatar</a> utilizes NPHM as a proxy geometry to add fine-grained control to powerful diffusion-based neural renderer.
                </p>
                <p>
                    <a href="https://tangjiapeng.github.io/projects/DPHMs/">DPHM</a> builds a diffusion prior for robust NPHM tracking using a depth sensor.
                </p><p>
                    <a href="https://raipranav384.github.io/clip_head">ClipHead</a> uses a large vision-language-model to add text control to NPHM.
                </p>
                <!--
                <p>
                     To be continued.
                </p>
                -->
            </div>
        </div>
    </div>
    </div>
    </section>






    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre>
<code>@misc{giebenhain2023mononphm,
    title={MonoNPHM: Dynamic Head Reconstruction from Monoculuar Videos},
    author={Simon Giebenhain and Tobias Kirschstein and Markos Georgopoulos and  Martin R{\"{u}}nz and Lourdes Agapito and Matthias Nie{\ss}ner},
    year={2023}
    publisher={arXiv},
    primaryClass={cs.CV}
}</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="static/nphm.pdf">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/SimonGiebenhain" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p style="text-align:center">
                            Source code borrowed from <a href="https://keunhong.com/">Keunhong Park</a>'s
                            <a href="https://nerfies.github.io/">Nerfies website</a> and especially
                            <a href="https://niessnerlab.org/members/yawar_siddiqui/profile.html">Yawar Siddiqui's</a> adoption for
                            <a href="https://nihalsid.github.io/texturify//">Texturify</a>.
                        </p>
                        <p style="text-align:center">
                            Please contact <a href="https://niessnerlab.org/members/simon_giebenhain/profile.html">Simon Giebenhain</a> for feedback and questions.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>




    <!--import { PLYLoader } from './js/PLYLoader.js'; -->
    <!-- Import maps polyfill         import { PLYLoader } from 'three/examples/jsm/loaders/PLYLoader'
-->
    <!-- Remove this when import maps will be widely supported -->
    <script async src="https://unpkg.com/es-module-shims@1.3.6/dist/es-module-shims.js"></script>

    <script type="importmap">
        {
            "imports": {
                "three": "./js/three.module.js"
            }
        }
    </script>

    <script type="module">

        import * as THREE from 'three';
        import { PLYLoader } from './js/PLYLoader.js';
        import { OrbitControls } from './js/OrbitControls.js'
        let div_to_scene = {
            "mesh_chair_1": {
                "geo": null,
            },
            "mesh_chair_2": {
                "geo": null,
            },
            "mesh_chair_3": {
                "geo": null,
            },
            "mesh_chair_4": {
                "geo": null,
            },
            "mesh_chair_5": {
                "geo": null,
            },
        }
        let mouse_button_down = false;
        let list_of_orbit_controls = []
        let style_camera = null;
        let render_colors = true;
        let style_id = "0"

        function setup_camera(div_name){
            let container = document.getElementById(div_name);
            let width = container.parentElement.clientWidth;
            let height = container.parentElement.clientHeight;
            console.log(width, height)
            let camera = new THREE.PerspectiveCamera( 35, width / height, 0.1, 50 );
            //let camera_init_position = new THREE.Vector3( -1.5, 0.35, 1.2 );
            let camera_init_position = new THREE.Vector3( 0., 0., 1.2 );
            camera_init_position = camera_init_position.multiplyScalar(1.5)
            //if (div_name.includes("style")) {
            //    camera_init_position = camera_init_position.multiplyScalar(1.25)
            //}
            //else if (div_name.includes("style")) {
            //    camera_init_position = camera_init_position.multiplyScalar(1.25)
            //}
            camera.position.set(camera_init_position.x, camera_init_position.y, camera_init_position.z);
            return camera;
        }

        function setup_render_divs(div_name, mesh_path){
            let camera = setup_camera(div_name)
            let orbit_control = create_render_div(camera, div_name, mesh_path)
            list_of_orbit_controls.push(orbit_control)
        }

        function create_render_div(camera, div_id, mesh_path) {
            let container;
            let renderer, controls;

            init();
            animate();

            function init() {

                container = document.getElementById(div_id);
                let width = container.parentElement.clientWidth;
                let height = container.parentElement.clientHeight;


                div_to_scene[div_id]["geo"] = new THREE.Scene();
                div_to_scene[div_id]["geo"].background = new THREE.Color( 0xffffff );
                //var axesHelper = new THREE.AxesHelper( 5 );
                //div_to_scene[div_id]["geo"].add( axesHelper );

                // PLY file

                const loader = new PLYLoader();
                loader.load( mesh_path, function ( geometry ) {

                    geometry.computeVertexNormals();
                    //let material_geo = new THREE.MeshStandardMaterial( { color: 0x444444, flatShading: true, roughness: 0.5, metalness: 0.3 } )
                    //let material_geo = new THREE.MeshPhongMaterial( { color: 0x999999, depthWrite: false} )
                    const material_geo = new THREE.MeshPhongMaterial( {
                                                color: 0xCCE6FF,
                                                specular: 0x222222,
                                                shininess: 25,} );

                    const mesh_geo = new THREE.Mesh( geometry, material_geo );
                    //mesh_geo.castShadow = true;
				    //mesh_geo.receiveShadow = true;
                    div_to_scene[div_id]["geo"].add( mesh_geo );

                }, (xhr) => {
                    console.log((xhr.loaded / xhr.total) * 100 + '% loaded')
                }, (error) => {
                    console.log(error)
                }
                );

                // lights

                //div_to_scene[div_id]["geo"].add( new THREE.HemisphereLight( 0x333333, 0x222222) );
                //div_to_scene[div_id]["geo"].add(new THREE.AmbientLight( 0x333333, 1) );
                //const directionalLight = new THREE.DirectionalLight( 0xffffff, 1. );
                //directionalLight.position.set( 1, 1, 1 );
                //div_to_scene[div_id]["geo"].add( directionalLight );
                //addShadowedLight(div_to_scene[div_id]["geo"], -5, -5, 5, 0xffffff, 1. );
                //addShadowedLight(div_to_scene[div_id]["geo"], 1, 1, 1, 0xffffff, 1.35 );
                //addShadowedLight(div_to_scene[div_id]["geo"], -1, 1, -1, 0xffffff, 1.35 );
                //addShadowedLight(div_to_scene[div_id]["geo"], 0, -2, 1, 0xffffff, 0.5 );
                //addShadowedLight(div_to_scene[div_id]["geo"],  0, -1, 2, 0xffffff, 0.5 );

                add_lights(div_to_scene[div_id]["geo"])




                // renderer

                renderer = new THREE.WebGLRenderer( { antialias: true } );
                renderer.setPixelRatio( window.devicePixelRatio );
                renderer.setSize( width, height);
                renderer.outputEncoding = THREE.sRGBEncoding;

                //renderer.shadowMap.enabled = true;

                container.appendChild( renderer.domElement );

                controls = new OrbitControls(camera, renderer.domElement)
                controls.enableDamping = false
                controls.autoRotate = true


                // resize

                window.addEventListener( 'resize', onWindowResize );
                //controls.addEventListener('change', onChange)
                controls.addEventListener('start', function(){
                  //clearTimeout(autorotateTimeout);
                  controls.autoRotate = false;
                });

                // restart autorotate after the last interaction & an idle time has passed
                //this.controls.addEventListener('end', function(){
                //  autorotateTimeout = setTimeout(function(){
                //    controls.autoRotate = true;
                //  }, 1000);
                //});

        }
            function onWindowResize() {
                let width = container.clientWidth;
                let height = container.clientHeight;
                camera.aspect = width / height;
                camera.updateProjectionMatrix();
                renderer.setSize( width, height );
            }
            function animate() {
                requestAnimationFrame( animate );
                controls.update();
                render();
            }

            function render() {
                renderer.render( div_to_scene[div_id]["geo"], camera );
                controls.update();
            }

            return controls;
        }

        function add_lights(scene) {
                scene.add( new THREE.HemisphereLight( 0x443333, 0x111122 ,0.05) );
				const spotLight = new THREE.SpotLight( 0xffffff, 0.5 );
				spotLight.position.set( 0.0, 0.5, 1 );
				const spotLight2 = new THREE.SpotLight( 0xffffff, 0.25 );
			    spotLight2.position.set( -2, 0.3, -0.2 );
				const spotLight3 = new THREE.SpotLight( 0xffffff, 0.25 );
				spotLight3.position.set( 2, -0.3, 0.2 );
				const spotLight4 = new THREE.SpotLight( 0xffffff, 0.25 );
				spotLight4.position.set( 0.0, 1, -2 );
				const spotLight5 = new THREE.SpotLight( 0xffffff, 0.05 );
				spotLight5.position.set( 0, -2.5, -1 );
				////spotLight.position.multiplyScalar( 700 );
				scene.add( spotLight );
				scene.add( spotLight2 );
				scene.add( spotLight3 );
				scene.add( spotLight4 );
				scene.add( spotLight5 );


        }

        function add_lightsBACKUP(scene) {
                scene.add( new THREE.HemisphereLight( 0x443333, 0x111122 ,0.05) );
				const spotLight = new THREE.SpotLight( 0xffffff, 0.6 );
				spotLight.position.set( 0.5, 0.5, 1 );
				const spotLight2 = new THREE.SpotLight( 0xffffff, 0.3 );
				spotLight2.position.set( -0.75, 0.2, 0 );
				const spotLight3 = new THREE.SpotLight( 0xffffff, 0.3 );
				spotLight3.position.set( -0.5, 0, 1 );
				const spotLight4 = new THREE.SpotLight( 0xffffff, 0.3 );
				spotLight4.position.set( 0.5, 0.5, -1 );
				const spotLight5 = new THREE.SpotLight( 0xffffff, 0.15 );
				spotLight5.position.set( 0, -1.5, 0 );
				////spotLight.position.multiplyScalar( 700 );
				scene.add( spotLight );
				scene.add( spotLight2 );
				//div_to_scene[div_id]["geo"].add( spotLight3 );
				scene.add( spotLight4 );
				scene.add( spotLight5 );
        }


        function addShadowedLight(scene, x, y, z, color, intensity ) {

            const directionalLight = new THREE.DirectionalLight( color, intensity );
            directionalLight.position.set( x, y, z );
            scene.add( directionalLight );

            directionalLight.castShadow = true;

            const d = 1;
            directionalLight.shadow.camera.left = - d;
            directionalLight.shadow.camera.right = d;
            directionalLight.shadow.camera.top = d;
            directionalLight.shadow.camera.bottom = - d;

            directionalLight.shadow.camera.near = 1;
            directionalLight.shadow.camera.far = 4;

            directionalLight.shadow.mapSize.width = 1024;
            directionalLight.shadow.mapSize.height = 1024;

            directionalLight.shadow.bias = - 0.001;

        }

        document.addEventListener('keydown', logKey);

        function logKey(evt) {
            if (evt.keyCode === 71 && !mouse_button_down) {
                switch_geometry()
            }
            if (evt.keyCode === 82 && !mouse_button_down) {
                reset_orbit_controls()
            }
        }

        function switch_geometry() {
            render_colors = !render_colors
        }

        function reset_orbit_controls() {
            list_of_orbit_controls.forEach(oc => {
                oc.reset()
                oc.autoRotate = false
            })
        }

        document.body.onmousedown = function(evt) {
            if (evt.button === 0)
                mouse_button_down = true
        }
        document.body.onmouseup = function(evt) {
            if (evt.button === 0)
                mouse_button_down = false
        }

        window.onload = function() {
            let slider = document.getElementsByClassName("slider")[0]
            slider.removeAttribute("tabIndex")
            // slider.addEventListener("mouseout", reset_orbit_controls);
            setup_render_divs("mesh_chair_1", './models/simon_510_s4.ply')
            setup_render_divs("mesh_chair_2", './models/simon_508_s5.ply')
            setup_render_divs("mesh_chair_3", './models/simon_510_s3.ply')
            setup_render_divs("mesh_chair_4", './models/simon_509_s4.ply')
            setup_render_divs("mesh_chair_5", './models/simon_511_s5.ply')
        };

    </script>
</body>

</html>

